{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we classify breast cancer tumours as malignant or benign with machine learning?\n",
    "\n",
    "Data from https://www.kaggle.com/uciml/breast-cancer-wisconsin-data \n",
    "\n",
    "Primary source: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\n",
    "Data citations:\n",
    "\n",
    "    Creators:\n",
    "\n",
    "    1. Dr. William H. Wolberg, General Surgery Dept.\n",
    "    University of Wisconsin, Clinical Sciences Center\n",
    "    Madison, WI 53792\n",
    "    wolberg '@' eagle.surgery.wisc.edu\n",
    "\n",
    "    2. W. Nick Street, Computer Sciences Dept.\n",
    "    University of Wisconsin, 1210 West Dayton St., Madison, WI 53706\n",
    "    street '@' cs.wisc.edu 608-262-6619\n",
    "\n",
    "    3. Olvi L. Mangasarian, Computer Sciences Dept.\n",
    "    University of Wisconsin, 1210 West Dayton St., Madison, WI 53706\n",
    "    olvi '@' cs.wisc.edu\n",
    "\n",
    "    Donor:\n",
    "\n",
    "    Nick Street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will implement logistic regression from scratch, and compare the results to those from an optimised implementation in the scikit-learn package.\n",
    "\n",
    "---\n",
    "\n",
    "# Import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r'data.csv')\n",
    "print(data.shape)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cleaning and preparing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "radius_mean                False\n",
      "texture_mean               False\n",
      "perimeter_mean             False\n",
      "area_mean                  False\n",
      "smoothness_mean            False\n",
      "compactness_mean           False\n",
      "concavity_mean             False\n",
      "concave points_mean        False\n",
      "symmetry_mean              False\n",
      "fractal_dimension_mean     False\n",
      "radius_se                  False\n",
      "texture_se                 False\n",
      "perimeter_se               False\n",
      "area_se                    False\n",
      "smoothness_se              False\n",
      "compactness_se             False\n",
      "concavity_se               False\n",
      "concave points_se          False\n",
      "symmetry_se                False\n",
      "fractal_dimension_se       False\n",
      "radius_worst               False\n",
      "texture_worst              False\n",
      "perimeter_worst            False\n",
      "area_worst                 False\n",
      "smoothness_worst           False\n",
      "compactness_worst          False\n",
      "concavity_worst            False\n",
      "concave points_worst       False\n",
      "symmetry_worst             False\n",
      "fractal_dimension_worst    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# need to split into target and features, as well as drop unnecessary columns\n",
    "targets = data.diagnosis\n",
    "features = data.drop(['id', 'diagnosis', 'Unnamed: 32'], axis = 1)\n",
    "\n",
    "# any missing data?\n",
    "print(targets.isnull().any())\n",
    "print(features.isnull().any())\n",
    "\n",
    "# no nulls, so no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100    1\n",
       "443    0\n",
       "312    0\n",
       "129    1\n",
       "444    1\n",
       "368    1\n",
       "262    1\n",
       "429    0\n",
       "31     1\n",
       "410    0\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes should be converted to numbers, 1 for malignant/0 for benign\n",
    "targets_num = targets.apply(lambda x: 0 if x == 'B' else 1)\n",
    "targets_num.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Scale the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets_num, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-1.149100</td>\n",
       "      <td>-0.390212</td>\n",
       "      <td>-1.127309</td>\n",
       "      <td>-0.957709</td>\n",
       "      <td>0.310642</td>\n",
       "      <td>-0.595339</td>\n",
       "      <td>-0.801714</td>\n",
       "      <td>-0.801608</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.094148</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.042029</td>\n",
       "      <td>0.213298</td>\n",
       "      <td>-1.034905</td>\n",
       "      <td>-0.847874</td>\n",
       "      <td>0.342122</td>\n",
       "      <td>-0.729295</td>\n",
       "      <td>-0.811427</td>\n",
       "      <td>-0.757150</td>\n",
       "      <td>-0.016130</td>\n",
       "      <td>-0.384611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>-0.936958</td>\n",
       "      <td>0.679766</td>\n",
       "      <td>-0.947159</td>\n",
       "      <td>-0.820622</td>\n",
       "      <td>-0.608966</td>\n",
       "      <td>-0.908867</td>\n",
       "      <td>-0.659943</td>\n",
       "      <td>-0.897728</td>\n",
       "      <td>0.754104</td>\n",
       "      <td>-0.425003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.714867</td>\n",
       "      <td>1.065669</td>\n",
       "      <td>-0.689163</td>\n",
       "      <td>-0.667962</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>-0.537275</td>\n",
       "      <td>-0.374636</td>\n",
       "      <td>-0.606203</td>\n",
       "      <td>0.096584</td>\n",
       "      <td>-0.385733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.573490</td>\n",
       "      <td>-1.032199</td>\n",
       "      <td>0.513376</td>\n",
       "      <td>0.408137</td>\n",
       "      <td>-0.106044</td>\n",
       "      <td>-0.362620</td>\n",
       "      <td>-0.417531</td>\n",
       "      <td>-0.088348</td>\n",
       "      <td>-0.271522</td>\n",
       "      <td>-0.574589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297288</td>\n",
       "      <td>-0.976743</td>\n",
       "      <td>0.261848</td>\n",
       "      <td>0.113763</td>\n",
       "      <td>-0.524147</td>\n",
       "      <td>-0.520294</td>\n",
       "      <td>-0.182788</td>\n",
       "      <td>-0.023693</td>\n",
       "      <td>-0.200282</td>\n",
       "      <td>-0.750616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.546618</td>\n",
       "      <td>-0.315675</td>\n",
       "      <td>-0.576987</td>\n",
       "      <td>-0.565992</td>\n",
       "      <td>0.586017</td>\n",
       "      <td>-0.648617</td>\n",
       "      <td>-0.804413</td>\n",
       "      <td>-0.499515</td>\n",
       "      <td>0.330714</td>\n",
       "      <td>0.539972</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.700554</td>\n",
       "      <td>-0.757093</td>\n",
       "      <td>-0.734928</td>\n",
       "      <td>-0.658241</td>\n",
       "      <td>-0.815850</td>\n",
       "      <td>-1.033783</td>\n",
       "      <td>-1.090433</td>\n",
       "      <td>-0.851607</td>\n",
       "      <td>-1.075002</td>\n",
       "      <td>-0.546282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-0.526818</td>\n",
       "      <td>0.790370</td>\n",
       "      <td>-0.560946</td>\n",
       "      <td>-0.522995</td>\n",
       "      <td>-1.050290</td>\n",
       "      <td>-1.016413</td>\n",
       "      <td>-0.904154</td>\n",
       "      <td>-0.934777</td>\n",
       "      <td>-0.968655</td>\n",
       "      <td>-0.426470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426556</td>\n",
       "      <td>1.057473</td>\n",
       "      <td>-0.421959</td>\n",
       "      <td>-0.440470</td>\n",
       "      <td>-0.303160</td>\n",
       "      <td>-0.466737</td>\n",
       "      <td>-0.723768</td>\n",
       "      <td>-0.782257</td>\n",
       "      <td>0.310898</td>\n",
       "      <td>-0.082039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "338    -1.149100     -0.390212       -1.127309  -0.957709         0.310642   \n",
       "427    -0.936958      0.679766       -0.947159  -0.820622        -0.608966   \n",
       "406     0.573490     -1.032199        0.513376   0.408137        -0.106044   \n",
       "96     -0.546618     -0.315675       -0.576987  -0.565992         0.586017   \n",
       "490    -0.526818      0.790370       -0.560946  -0.522995        -1.050290   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "338         -0.595339       -0.801714            -0.801608       0.294215   \n",
       "427         -0.908867       -0.659943            -0.897728       0.754104   \n",
       "406         -0.362620       -0.417531            -0.088348      -0.271522   \n",
       "96          -0.648617       -0.804413            -0.499515       0.330714   \n",
       "490         -1.016413       -0.904154            -0.934777      -0.968655   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "338                0.094148  ...     -1.042029       0.213298   \n",
       "427               -0.425003  ...     -0.714867       1.065669   \n",
       "406               -0.574589  ...      0.297288      -0.976743   \n",
       "96                 0.539972  ...     -0.700554      -0.757093   \n",
       "490               -0.426470  ...     -0.426556       1.057473   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "338        -1.034905   -0.847874          0.342122          -0.729295   \n",
       "427        -0.689163   -0.667962         -0.095432          -0.537275   \n",
       "406         0.261848    0.113763         -0.524147          -0.520294   \n",
       "96         -0.734928   -0.658241         -0.815850          -1.033783   \n",
       "490        -0.421959   -0.440470         -0.303160          -0.466737   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "338        -0.811427             -0.757150       -0.016130   \n",
       "427        -0.374636             -0.606203        0.096584   \n",
       "406        -0.182788             -0.023693       -0.200282   \n",
       "96         -1.090433             -0.851607       -1.075002   \n",
       "490        -0.723768             -0.782257        0.310898   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "338                -0.384611  \n",
       "427                -0.385733  \n",
       "406                -0.750616  \n",
       "96                 -0.546282  \n",
       "490                -0.082039  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale features, keeping mean and standard deviation for later scaling of test data\n",
    "x_scaler_mean = x_train.mean()\n",
    "x_scaler_std = x_train.std()\n",
    "x_train = (x_train - x_scaler_mean) / x_scaler_std\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.201534</td>\n",
       "      <td>0.328717</td>\n",
       "      <td>-0.130724</td>\n",
       "      <td>-0.271157</td>\n",
       "      <td>1.028066</td>\n",
       "      <td>0.863168</td>\n",
       "      <td>0.732832</td>\n",
       "      <td>0.855755</td>\n",
       "      <td>1.119096</td>\n",
       "      <td>1.551877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.675545</td>\n",
       "      <td>0.185082</td>\n",
       "      <td>-0.062739</td>\n",
       "      <td>1.102317</td>\n",
       "      <td>0.873481</td>\n",
       "      <td>1.217750</td>\n",
       "      <td>1.387802</td>\n",
       "      <td>1.080843</td>\n",
       "      <td>1.538603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>-0.255277</td>\n",
       "      <td>1.466020</td>\n",
       "      <td>-0.317455</td>\n",
       "      <td>-0.323646</td>\n",
       "      <td>-0.616212</td>\n",
       "      <td>-1.015423</td>\n",
       "      <td>-0.768167</td>\n",
       "      <td>-0.725696</td>\n",
       "      <td>-0.694912</td>\n",
       "      <td>-1.001348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.389751</td>\n",
       "      <td>1.424648</td>\n",
       "      <td>-0.464771</td>\n",
       "      <td>-0.423417</td>\n",
       "      <td>-0.157309</td>\n",
       "      <td>-0.950705</td>\n",
       "      <td>-0.643623</td>\n",
       "      <td>-0.832777</td>\n",
       "      <td>-0.730512</td>\n",
       "      <td>-0.876361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.026164</td>\n",
       "      <td>-0.839844</td>\n",
       "      <td>-0.091650</td>\n",
       "      <td>-0.132115</td>\n",
       "      <td>-1.214066</td>\n",
       "      <td>-0.940952</td>\n",
       "      <td>-0.856211</td>\n",
       "      <td>-0.574391</td>\n",
       "      <td>-0.804409</td>\n",
       "      <td>-0.998415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275244</td>\n",
       "      <td>-1.021000</td>\n",
       "      <td>-0.310058</td>\n",
       "      <td>-0.327918</td>\n",
       "      <td>-1.284343</td>\n",
       "      <td>-0.980749</td>\n",
       "      <td>-1.026317</td>\n",
       "      <td>-0.494294</td>\n",
       "      <td>-1.219466</td>\n",
       "      <td>-0.920147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.041721</td>\n",
       "      <td>-0.236328</td>\n",
       "      <td>-0.024608</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>-2.221359</td>\n",
       "      <td>-1.012452</td>\n",
       "      <td>-0.806341</td>\n",
       "      <td>-0.905501</td>\n",
       "      <td>-0.636513</td>\n",
       "      <td>-0.998415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007381</td>\n",
       "      <td>-0.045692</td>\n",
       "      <td>-0.036358</td>\n",
       "      <td>-0.104861</td>\n",
       "      <td>-1.679909</td>\n",
       "      <td>-0.229652</td>\n",
       "      <td>-0.555964</td>\n",
       "      <td>-0.599314</td>\n",
       "      <td>-0.425709</td>\n",
       "      <td>-0.377874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.306191</td>\n",
       "      <td>-0.176217</td>\n",
       "      <td>-0.378739</td>\n",
       "      <td>-0.363013</td>\n",
       "      <td>-0.477076</td>\n",
       "      <td>-1.300430</td>\n",
       "      <td>-0.795159</td>\n",
       "      <td>-0.504179</td>\n",
       "      <td>-1.249699</td>\n",
       "      <td>-0.592187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604450</td>\n",
       "      <td>-0.447289</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.571781</td>\n",
       "      <td>-1.566764</td>\n",
       "      <td>-1.343300</td>\n",
       "      <td>-1.097796</td>\n",
       "      <td>-0.984643</td>\n",
       "      <td>-1.456006</td>\n",
       "      <td>-1.223841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "512    -0.201534      0.328717       -0.130724  -0.271157         1.028066   \n",
       "457    -0.255277      1.466020       -0.317455  -0.323646        -0.616212   \n",
       "439    -0.026164     -0.839844       -0.091650  -0.132115        -1.214066   \n",
       "298     0.041721     -0.236328       -0.024608  -0.057848        -2.221359   \n",
       "37     -0.306191     -0.176217       -0.378739  -0.363013        -0.477076   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "512          0.863168        0.732832             0.855755       1.119096   \n",
       "457         -1.015423       -0.768167            -0.725696      -0.694912   \n",
       "439         -0.940952       -0.856211            -0.574391      -0.804409   \n",
       "298         -1.012452       -0.806341            -0.905501      -0.636513   \n",
       "37          -1.300430       -0.795159            -0.504179      -1.249699   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "512                1.551877  ...      0.031470       0.675545   \n",
       "457               -1.001348  ...     -0.389751       1.424648   \n",
       "439               -0.998415  ...     -0.275244      -1.021000   \n",
       "298               -0.998415  ...     -0.007381      -0.045692   \n",
       "37                -0.592187  ...     -0.604450      -0.447289   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "512         0.185082   -0.062739          1.102317           0.873481   \n",
       "457        -0.464771   -0.423417         -0.157309          -0.950705   \n",
       "439        -0.310058   -0.327918         -1.284343          -0.980749   \n",
       "298        -0.036358   -0.104861         -1.679909          -0.229652   \n",
       "37         -0.666429   -0.571781         -1.566764          -1.343300   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "512         1.217750              1.387802        1.080843   \n",
       "457        -0.643623             -0.832777       -0.730512   \n",
       "439        -1.026317             -0.494294       -1.219466   \n",
       "298        -0.555964             -0.599314       -0.425709   \n",
       "37         -1.097796             -0.984643       -1.456006   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "512                 1.538603  \n",
       "457                -0.876361  \n",
       "439                -0.920147  \n",
       "298                -0.377874  \n",
       "37                 -1.223841  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for x_test, remember to scale by the mean and std dev. of the training set:\n",
    "x_test = (x_test - x_scaler_mean) / x_scaler_std\n",
    "\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Implementing unregularised logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_logistic_regression(features, targets, alpha = 0.001, epsilon = 0.001, count = 1000, plot = False):\n",
    "    \"\"\"\n",
    "    trains an unregularised logistic regression model on the given features and targets, returning the optimised theta parameters\n",
    "    'alpha' is the learning rate\n",
    "    'epsilon' is the change in J(theta) between iterations below which we will consider it to have converged\n",
    "    'count' is the maximum number of iterations to run gradient descent for (in the case it doesn't converge before that)\n",
    "    'plot' will plot the graph of cost vs number of iterations if toggled to True, useful for troubleshooting.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    # add intercept term to the features\n",
    "    features['intercept'] = 1\n",
    "    # rearrange so intercept term is first column\n",
    "    features = features[['intercept']+features.columns.tolist()[:-1]] \n",
    "    \n",
    "    # initialise theta as 0s\n",
    "    init_theta = np.array([np.zeros(features.shape[1])]).T # produces a numpy array of shape nx1 (as opposed to (n,))\n",
    "      \n",
    "    # implement gradient descent\n",
    "    def grad_desc(X, y, theta, alpha, epsilon, count):\n",
    "        \"\"\"\n",
    "        implements gradient descent to optimise theta by minimising the cost function\n",
    "        \"\"\"\n",
    "        \n",
    "        def sigmoid(z):\n",
    "            \"\"\"\n",
    "            maps any z (matrix, vector, or scalar) to the (0, 1) interval, using a sigmoid curve\n",
    "            \"\"\"\n",
    "\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "        # define cost function for logistic regression\n",
    "        def lr_cost(theta, X, y):\n",
    "            \"\"\"\n",
    "            vectorised implementation of cost function for logistic regression, returning the cost for given theta parameters\n",
    "            \"\"\"\n",
    "\n",
    "            m = len(y)\n",
    "            h = sigmoid(X.dot(theta)) # mx31 * 31x1 = mx1 vector (31 = num. features + intercept term)\n",
    "            y_tp = y.T # transposes y from mx1 to a 1xm vector (horizontal)\n",
    "            J = 1/m * ((-y_tp.dot(np.log(h))) - ((1-y).T.dot(np.log(1-h)))) # 1xm * mx1 = 1x1\n",
    "\n",
    "            # uncomment following if debugging:\n",
    "    #         print('\\ndebugging lr_cost:')\n",
    "    #         print('h should be mx1, h is:', h.shape)\n",
    "    #         print('y should be mx1, y is:', y.shape)\n",
    "    #         print('y_tp should be 1xm, is:', y_tp.shape)\n",
    "    #         print('-y_tp*log(h) should be 1x1, is:', (-y_tp.dot(np.log(h))).shape)\n",
    "    #         print('(1-y).T should be 1xm, is:', (1-y).T.shape)\n",
    "\n",
    "            return J\n",
    "    \n",
    "        y = np.array([targets]).T\n",
    "        m = len(y)\n",
    "        costs = []\n",
    "        iterations = []\n",
    "\n",
    "        for i in range(count):\n",
    "            \n",
    "            # calculate change in theta\n",
    "            delta = (X.T.dot(X.dot(theta) - y))/m # 31xm * (mx31 * 31x1 = mx1) = 31x1 vector\n",
    "            \n",
    "            # update theta\n",
    "            theta = theta - alpha * delta \n",
    "\n",
    "            # calculate and store cost with new theta\n",
    "            new_cost = lr_cost(theta, X, y)[0][0]\n",
    "            costs.append(new_cost)\n",
    "            iterations.append(i)\n",
    "\n",
    "            # stop gradient descent if difference in costs between steps is less than epsilon\n",
    "            if len(costs) > 1:\n",
    "                if costs[-2] - new_cost <= epsilon:\n",
    "                    break\n",
    "                    \n",
    "            # uncomment if debugging:\n",
    "#            print('\\ndebugging grad_desc:')\n",
    "#            print('X should be mx31, X:', X.shape)\n",
    "#            print('X.T should be 31xm, X.T:', X.T.shape)\n",
    "#            print('theta should be 31x1, theta:', theta.shape)\n",
    "#            print('y should be mx1, y:', y.shape)\n",
    "#            print('X * theta should be mx1, X * theta:', X.dot(theta).shape)\n",
    "#            print('X * theta - y should be mx1, X * theta - y:', (X.dot(theta) - y).shape)\n",
    "#            print('delta should be 31x1, delta:', delta.shape)\n",
    "\n",
    "        return theta, iterations, costs\n",
    "\n",
    "    final_theta, iterations, final_costs = grad_desc(features, targets, init_theta, alpha = alpha, epsilon = epsilon, count = count)\n",
    "    \n",
    "    # plot cost against theta if toggled\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iterations, final_costs)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('cost')\n",
    "        plt.show()\n",
    "        \n",
    "    return final_theta\n",
    "\n",
    "lr_model = train_logistic_regression(x_train, y_train, alpha = 0.003, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot shows steady decrease in cost as expected, a successful convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512    1\n",
       "457    0\n",
       "439    0\n",
       "298    0\n",
       "37     0\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the optimised theta parameters to predict classes of test data\n",
    "\n",
    "def logistic_regression_predictor(features, lr_model, boundary = 0.5):\n",
    "    \"\"\"\n",
    "    uses the given trained logistic regression model (optimised theta parameters) to predict classes from the given features\n",
    "    'boundary' is the value (float between 0 and 1) above which to predict malignancy\n",
    "    \"\"\"\n",
    "    \n",
    "    # add intercept term to the features\n",
    "    features['intercept'] = 1\n",
    "    # rearrange so intercept term is first column \n",
    "    features = features[['intercept']+features.columns.tolist()[:-1]]    \n",
    "    \n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        maps any z (matrix, vector, or scalar) to the (0, 1) interval, using a sigmoid curve\n",
    "        \"\"\"\n",
    "        from numpy import exp\n",
    "        \n",
    "        return 1 / (1 + exp(-z))\n",
    "    \n",
    "    # use the logistic regression model to map the features to the (0, 1) interval\n",
    "    lr_y_mapped = sigmoid(features.dot(lr_model))\n",
    "    \n",
    "    # convert the result to a prediction of class based on the chosen boundary\n",
    "    lr_y_pred = lr_y_mapped[0].apply(lambda x: 1 if x >= boundary else 0)\n",
    "    \n",
    "    return lr_y_pred\n",
    "\n",
    "lr_y_pred = logistic_regression_predictor(x_test, lr_model)\n",
    "lr_y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# How accurate is the model at predicting the classes of new data?\n",
    "\n",
    "A common way to measure the accuracy of predictive models is with confusion matrices. These show the number of true positives (predict 1, actually 1), false positives (predict 1, actually 0), false negatives (predict 0, actually 1) and true negatives (predict 0, actually 0). Confusion matrices are arranged as follows:\n",
    "\n",
    "                                        TP FP\n",
    "\n",
    "                                        FN TN\n",
    "\n",
    "These numbers can be used to calculate metrics such as **precision** (what proportion of data points classified as the positive class were actually positive) and **recall** (what proportion of actual positives were identified as positive). A good model needs a high score in both, which can be measured by the **f1 score**, their harmonic mean. All of these metrics run from 0 (lowest) to 1 (highest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[54 13]\n",
      " [ 1 46]]\n"
     ]
    }
   ],
   "source": [
    "# compare with actual results\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89        67\n",
      "           1       0.78      0.98      0.87        47\n",
      "\n",
      "    accuracy                           0.88       114\n",
      "   macro avg       0.88      0.89      0.88       114\n",
      "weighted avg       0.90      0.88      0.88       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate precision/recall/f1-score\n",
    "print(classification_report(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a final f1 score of 0.88 - not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adding regularisation to the logistic regression model:\n",
    "\n",
    "Overfitting to the training data can sometimes negatively affect the accuracy of the final model. Even though our f1 score is pretty decent already, it may improve if we account for this. **Regularisation** is used to help avoid this overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: difference between this code and the prior is the addition of the regularisation term to J in the lr_cost function\n",
    "\n",
    "def train_reg_logistic_regression(features, targets, lambda_reg, alpha = 0.001, epsilon = 0.001, count = 1000, plot = False):\n",
    "    \"\"\"\n",
    "    trains a regularised logistic regression model on the given features and targets, returning the optimised theta parameters\n",
    "    'lambda_reg' is the regularisation parameter\n",
    "    'alpha' is the learning rate\n",
    "    'epsilon' is the change in J(theta) between iterations below which we will consider it to have converged\n",
    "    'count' is the maximum number of iterations to run gradient descent for (in the case it doesn't converge before that)\n",
    "    'plot' will plot the graph of cost vs number of iterations if toggled to True, useful for troubleshooting.\n",
    "    \"\"\"\n",
    "        \n",
    "    import numpy as np\n",
    "\n",
    "    # add intercept term to the features\n",
    "    features['intercept'] = 1\n",
    "    # rearrange so intercept term is first column\n",
    "    features = features[['intercept'] + features.columns.tolist()[:-1]] \n",
    "      \n",
    "    # initialise theta as 0s\n",
    "    init_theta = np.array([np.zeros(features.shape[1])]).T # produces an array of shape nx1 (as opposed to (31,))\n",
    "      \n",
    "    # implement gradient descent\n",
    "    def grad_desc(X, y, theta, lambda_reg, alpha, epsilon, count):\n",
    "        \"\"\"\n",
    "        implements gradient descent to optimise theta by minimising the cost function\n",
    "        \"\"\"\n",
    "        \n",
    "        def sigmoid(z):\n",
    "            \"\"\"\n",
    "            maps any z (matrix, vector, or scalar) to the (0, 1) interval, using a sigmoid curve\n",
    "            \"\"\"\n",
    "\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "        # define cost function for logistic regression\n",
    "        def lr_cost(theta, X, y, lambda_reg):\n",
    "            \"\"\"\n",
    "            vectorised implementation of REGULARISED cost function for logistic regression\n",
    "            this calculates both the cost of some particular theta, as well as the gradient for use in gradient descent\n",
    "            \"\"\"\n",
    "\n",
    "            m = len(y)\n",
    "            h = sigmoid(X.dot(theta)) # mx31 * 31x1 = mx1 vector (31 = num. features + intercept term)\n",
    "            y_tp = y.T # transposes y from mx1 to a 1xm vector (horizontal)\n",
    "            \n",
    "            # calculate cost, this time WITH regularisation term\n",
    "            J = 1/m * ((-y_tp.dot(np.log(h))) - ((1-y).T.dot(np.log(1-h)))) + lambda_reg/(2*m) * sum(theta[1:].values**2) # 1xm * mx1 = 1x1\n",
    "                        \n",
    "            return J\n",
    "    \n",
    "        y = np.array([targets]).T\n",
    "        m = len(y)\n",
    "        costs = []\n",
    "        iterations = []\n",
    "\n",
    "        for i in range(count):\n",
    "            \n",
    "            delta = (X.T.dot(X.dot(theta) - y))/m # 31xm * (mx31 * 31x1 = mx1) = 31x1 vector\n",
    "            theta = theta - alpha * delta # update theta\n",
    "\n",
    "            # calculate and store cost with new theta\n",
    "            new_cost = lr_cost(theta, X, y, lambda_reg)[0][0]\n",
    "            costs.append(new_cost)\n",
    "            iterations.append(i)\n",
    "\n",
    "            # stop gradient descent if difference in costs between steps is less than epsilon\n",
    "            if len(costs) > 1:\n",
    "                if costs[-2] - new_cost <= epsilon:\n",
    "                    break\n",
    "\n",
    "        return theta, iterations, costs\n",
    "    \n",
    "    final_theta, iterations, final_costs = grad_desc(features, targets, init_theta, lambda_reg=lambda_reg, alpha=alpha, epsilon=epsilon, count=count)\n",
    "    \n",
    "    # plot cost against theta if toggled\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iterations, final_costs)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('cost')\n",
    "        plt.show()\n",
    "        \n",
    "    return final_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of the regularisation parameter *lambda_reg* is determined through testing various possible values on a cross-validation set, and choosing the one that minimises the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 114, 114)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into train, test, and cross-validation sets (for determining the value of the regularisation parameter)\n",
    "from sklearn.model_selection import train_test_split # if haven't imported already\n",
    "\n",
    "reg_x_train, x_temp, reg_y_train, y_temp = train_test_split(features, targets_num, test_size = 0.4, random_state = 0)\n",
    "reg_x_cv, reg_x_test, reg_y_cv, reg_y_test = train_test_split(x_temp, y_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "# scale all feature sets (using mean and std dev. of the training set)\n",
    "reg_x_scaler_mean = reg_x_train.mean()\n",
    "reg_x_scaler_std = reg_x_train.std()\n",
    "\n",
    "reg_x_train = (reg_x_train - reg_x_scaler_mean) / reg_x_scaler_std\n",
    "reg_x_cv = (reg_x_cv - reg_x_scaler_mean) / reg_x_scaler_std\n",
    "reg_x_test = (reg_x_test - reg_x_scaler_mean) / reg_x_scaler_std\n",
    "\n",
    "len(reg_x_train), len(reg_x_cv), len(reg_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine value of regularisation parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06140350877192982"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train models with various values of regularisation parameter on the training data\n",
    "lambda_regs = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "models = []\n",
    "\n",
    "for i in lambda_regs:\n",
    "    models.append(train_reg_logistic_regression(reg_x_train, reg_y_train, lambda_reg = 10**i, alpha = 0.003))\n",
    "\n",
    "# make predictions based on each of these models    \n",
    "predictions = []\n",
    "for model in models:\n",
    "    predictions.append(logistic_regression_predictor(reg_x_cv, model))\n",
    "    \n",
    "# determine best regularisation parameter from cross-validation set  \n",
    "def error_calc(prediction, actual):\n",
    "    \"\"\"\n",
    "    calculates the error in the given predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(prediction)\n",
    "    J = 1/(2*m) * sum((prediction - actual)**2)\n",
    "    \n",
    "    return J\n",
    "    \n",
    "errors = []\n",
    "for i in predictions:\n",
    "    errors.append(error_calc(i, reg_y_cv))\n",
    "\n",
    "# find model, and therefore regularisation parameter, that led to the lowest error    \n",
    "min_reg_error = errors.index(min(errors))\n",
    "lambda_choice = lambda_regs[min_reg_error]\n",
    "model_choice = models[min_reg_error]\n",
    "\n",
    "# calculate final model error on TEST set (not cross-validation, so result is properly generalised to new data)\n",
    "final_pred = logistic_regression_predictor(reg_x_test, model_choice)\n",
    "\n",
    "error_calc(final_pred, reg_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[58 10]\n",
      " [ 4 42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89        68\n",
      "           1       0.81      0.91      0.86        46\n",
      "\n",
      "    accuracy                           0.88       114\n",
      "   macro avg       0.87      0.88      0.87       114\n",
      "weighted avg       0.88      0.88      0.88       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how accurate is the regularised model?\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(reg_y_test, final_pred))\n",
    "print(classification_report(reg_y_test, final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the addition of regularisation did not change the final f1 score of the model. This suggests that overfitting is not the main source of error.\n",
    "\n",
    "---\n",
    "# Changing the decision boundary\n",
    "\n",
    "In this context of diagnosing breast cancer tumour malignancy, it can be argued that false positives (AKA type I error) are not as bad as false negatives (AKA type II error). A false positive result would be likely to lead to further tests to confirm the diagosis, while a false negative could lead to actually dangerous tumours being overlooked until it is too late.\n",
    "\n",
    "As such, we can change the position of the decision boundary to err on the side of caution and avoid false negatives (predict malignancy more often), at the cost of more false positives and a worse overall f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[45 23]\n",
      " [ 0 46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.66      0.80        68\n",
      "           1       0.67      1.00      0.80        46\n",
      "\n",
      "    accuracy                           0.80       114\n",
      "   macro avg       0.83      0.83      0.80       114\n",
      "weighted avg       0.87      0.80      0.80       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changed_boundary_pred = logistic_regression_predictor(reg_x_test, model_choice, boundary = 0.47)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(reg_y_test, changed_boundary_pred))\n",
    "print(classification_report(reg_y_test, changed_boundary_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Using optimised logistic regression implementation from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets_num, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# scale both feature sets\n",
    "x_train = (x_train - x_train.mean()) / x_train.std()\n",
    "x_test = (x_test - x_test.mean()) / x_test.std()\n",
    "\n",
    "# train sklearn logistic regression model\n",
    "sklearn_lr = LogisticRegression(solver = 'lbfgs')\n",
    "sklearn_lr.fit(x_train, y_train)\n",
    "\n",
    "# predict test set with model\n",
    "sklearn_y_pred = sklearn_lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[65  2]\n",
      " [ 3 44]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, sklearn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        67\n",
      "           1       0.96      0.94      0.95        47\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, sklearn_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably better than our code written from scratch (f1 score of 0.96 compared to our 0.88). An argument for not reinventing the wheel perhaps! Their results do have some false negatives however, which as we discussed are to be avoided!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
